pr## Introducción al análisis de algoritmos
### Eficiencia de los algoritmos
- Estudio teórico comparativo
- Independiente de la implementación
- Clasificar algoritmos en familias
- Comportamiento según crece el tamaño de su entrada -> asintótico

### Cómo medimos la eficiencia
- Coste temporal
- Caso peor
- -> Coste asintótico temporal en el caso peor

### Análisis de coste
- Métricas de análisis: clasificación de funciones en familias
- Reglas prácticas para el análisis:
	- Programas iterativos
	- Programas recursivos

## Métricas de análisis y órdenes de complejidad

### Notación de Landau

#### Cota superior

$O(f) =$ {$g$ / $g$ no crece más deprisa que $f$}

Formalmente: 

$O(f)=\left\{g:ℕ→ℝ^{+}∪~\{0\}∣∃n_0∈ℕ,c∈ℝ^{+} t. q.∀ n≥n0 \cdot g(n)≤c⋅f(n)\right\}$ 


#### Cota inferior

$\Omega(f) =$ {$g$ / $g$ crece al menos como $f$}

Formalmente:

$\Omega(f)=\left\{g:ℕ→ℝ^{+}∪~\{0\}∣∃n_0∈ℕ,c∈ℝ^{+} t. q.∀ n≥n0 \cdot g(n)≥c⋅f(n)\right\}$

#### Cota exacta

$\Theta(f) =$ {$g$ / $g$ crece exactamente como $f$}

Formalmente:

$\Theta(f) = O(f) ∩ \Omega(f)$ 

### Órdenes de complejidad

#### Jerarquía de órdenes

$O(1)⊂O(log_n )⊂O(\sqrt{n})⊂O(n)⊂O(n⋅log_n )⊂O(n^{2})⊂$
$⊂O(n³)⊂~...~⊂O(n^k)⊂O(2^n)⊂~...~⊂O(k^n)⊂O(n!)⊂O(n^n)$

Constante < Logarítmico < Cuadrático inverso < Lineal <	Cuasilineal < Cuadrático < Cúbico < ... < Polinómico de grado k < Exponencial < ... < Exponencial de grado k < Factorial < Hiperexponencial


## Reglas prácticas para el cálculo del coste
### Algoritmos iterativos
- Operaciones básicas -> conste costante O(1)
	- Operaciones de entrada / salida
	- Asignación
	- Expresiones
- Composición secuencial 
	$c(S_1 )∈O(cS_1) ~ c(S_2 )∈O(cS_2)~ ... ~ c(S_n )∈O(cS_n)$ 
	
	- Suma de órdenes $O(cS_1) + O(cS_2) + ... + O(cS_n) = O(cS_1 + cS_2 + ... + cS_n) = max_i \left\{O(cS_i)\right\}$
- Sentencias condicionales
	if (e) {$S_1$} else {$S_2$} 
	Coste = coste máximo de e;$S_1$ y e;$S_2$ 
	$max\left\{O(c_e), O(c_{S1}), O(c_{S2})\right\}$ 
	similar para switch
- Bucles
	- for (ini; e; inc) {S}
		$max\left\{O(c_{ini}), O(v(n))·max{O(c_e),O(c_S),O(c_{inc})}\right\}$ 
	- while (e) {S} // do {S} while (e)
		$O(v(n))·max\left\{O(c_e ),O(c_S)\right\}$
- Producto de órdenes
		$O(f) · O(g) = O(f·g)$ 
- Llamada a subprogramas: f(e1,e2, … , en)
	$max\left\{O(c_{e1}),O(c_{e2}),...,O(c_{en}),O(c_f)\right\}$
	
### Algoritmos recursivos

#### Recurrencias
En un algoritmo recursivo, su función de coste también es recursiva, se expresa como:
$$
\begin{equation}
  T_n =
    \begin{cases}
      k_1 &  \text{si $n$ es caso base}\\
      a\cdot T(n')+k_2(n) & \text{si $n$ no es caso base}\\
    \end{cases}       
\end{equation}
$$
A esto se le llama recurrencia.

#### Resolución de recurrencias
##### Reducción mediante sustracción
$$
\begin{equation}
  T(n) =
    \begin{cases}
      c_b(n) &  \text{si $0 \leq n \leq b$}\\
      a\cdot T(n - b)+c_{nr}(n) & \text{si $n \geq b$}\\
    \end{cases}       
\end{equation}
$$
- $c_b(n)$ -> coste del caso base
- $c_{nr}(n)$ -> coste de las operaciones no recursivas en cada llamada
- $a$ -> número de llamadas recursivas
- $n - b$ -> tamaño de los subproblemas

$$
\begin{equation}
  T_n =
    \begin{cases}
      O(n \cdot c_{nr}(n) + c_b(n)) &  \text{si $a = 1$}\\
      O(a^{(n ~ \text{div} ~ b)} \cdot (c_{nr}(n) + c_b(n))) & \text{si $a \gt 1$}\\
    \end{cases}       
\end{equation}
$$


##### Reducción mediante división
$$
\begin{equation}
  T(n) =
    \begin{cases}
      c_b(n) &  \text{si $0 \leq n \leq b$}\\
      a\cdot T(n ~ \text{div} ~ b)+c_{nr}(n) & \text{si $n \geq b$}\\
    \end{cases}       
\end{equation}
$$

$$
\begin{equation}
  T_n =
    \begin{cases}
      O(\log_b(n) \cdot c_{nr}(n) + c_b(n)) &  \text{si $a = 1$}\\
      O(n^{(\log_b(a))} \cdot (c_{nr}(n) + c_b(n))) & \text{si $a \gt 1$}\\
    \end{cases}       
\end{equation}
$$
