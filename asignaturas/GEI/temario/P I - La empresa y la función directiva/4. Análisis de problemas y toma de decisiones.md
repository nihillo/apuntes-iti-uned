## La modelización
- Modelo: representación simplificada de una parte de la realidad
- Clasificaciones de modelos
	- Objetivos y subjetivos
	- Analíticos y de simulación
	- Estáticos y dinámicos
	- Deterministas y probabilísticos
## Ambientes de decisión
- Clasificados según nivel de información que comportan
- Ambientes de decisión
	- Certeza
		- Se conocen con seguridad los estados de la naturaleza que van a presentarsre
	- Riesgo
		- Se conocen los estados que pueden presentarse y su probabilidad
	- Incertidumbre estructurada
		- Se conocen los estados que pueden presentarse pero no su probabilidad
	- Incertidumbre no estructurada
		- No se conocen los estados posibles
## Criterios de decisión en ambiente de incertidumbre
- Incertidumbre no estructurada
	- Mera intuición
- Incertidumbre estructurada
	- Laplace
		- también llamado criterio racionalista o de igual verosimilitud
		- Se asigna igual probabilidad a cada uno de los posibles estados de la naturaleza
		- Se calcula el resultado de cada decisión como la media aritmética de sus posibles resultados
	- Optimista
		- hipótesis del mejor resultado de los posibles para la estrategia elegida
		- resultados favorables -> maxi-max 
			- maxi-max: máximo de los máximos. se determina el resultado más elevado para cada estrategia, y luego se elige aquella a la que corresponda el máximo de estos máximos
		- resultados desfavorables -> mini-min
			- mini-min: mínimo de los mínimos. se determinan los mínimos y se elige el mínimo de entre ellos
	- Pesimista (Wald)
		- hipótesis del peor resultado de los posibles para la estrategia elegida
		- resultados favorables -> maxi-min (criterio prudente)
			- se elige el máximo de entre los mínimos
		- resultados desfavorables -> mini-max
			- mínimo de entre los máximos
	- Optimismo parcial (Hurwicz)
		- compromiso entre optimista y pesimista
		- coeficientes de optimismo y pesimismo (compementarios, de 0 a 1)
		- mejor resultado se pondera con el coeficiente de optimismo, peor resultado con el de pesimismo
	- Mínimo pesar (Savage)
		- menor arrepentimiento por equivocación
		- se parte de la matriz de pesares, se determina para cada estrategia el mayor pesar, y se elige la que tenga el menor de estos máximos (mini-max)
- Estrategia dominada
	- A está dominada por B si sea cual sea el estado de la naturaleza que se presente B es igual o mejor que A
	- Las estrategias dominadas se descartan en primer lugar
## La teoría de los juegos de estrategia
- Resultado final depende no solo de nuestras decisiones sino también de las de otros
- Clasificación
	- Número de participantes
	- Ganancia total obtenida
		- Suma nula
			- Total ganado por unos es igual a total perdido por otros
		- Suma no nula
			- Constante
			- Variable
	- Número de jugadas
		- Una
		- Varias
		- Infinitas
	- Información disponible
		- Información completa
		- Información incompleta
	- Elementos que intervengan en las decisiones
		- Estrategia pura
			- Sólo interviene la actuación de los jugadores
		- Estrategia mixta
			- Interviene algún elemento aleatorio
- Juego rectangular (juego de dos personas de suma nula)
	- Valor del juego
		- Cantidad que gana un jugador y el otro pierde
	- Si los jugadores aplican criterio de Wald, el ganador elegirá el maxi-min y el perdedor el mini-max
	- Punto de silla
		- Valor que es a la vez el maxi-min del ganador y el mini-max del perdedor
		- Puede haber juegos con un punto de silla, más de uno, o ninguno
## Probabilidad y riesgo
- Concepciones sobre la probabilidad
	- Definición de Laplace
		- La probabilidad de un suceso es el cociente entre el número de casos favorables y el de casos posibles
	- Concepción frecuencial u objetivista de la probabilidad
		- Probabilidad estimada, empírica, como límite de la frecuencia del suceso cuando las observaciones tienden a infinito
- Suceso compuesto
	- Dos sucesos $S$ y $T$ -> suceso compuesto es que ocurran los dos: $S \cap T$ 
	- $P(S \cap T) = P(T)P(S|T) = P(S)P(T|S)$
		- $P(S|T)$ -> Probabilidad de $S$ condicionada a $T$, es decir, probabilidad de que suceda $S$ dado que $T$ haya sucedido
	- Sucesos independientes entre sí
		- $P(S|T) = P(S)$
		- $P(T|S) = P(T)$
- Sucesos mutuamente excluyentes
	- Sucesos que no se pueden dar simultaneamente
	- Caso general
		- $P(S \cup T) = P(S) + P(T) - P(S \cap T)$ 
	- Mutuamente excluyentes
		- $P(S \cup T) = P(S) + P(T); P(S \cap T) = 0$ 
- Variable aleatoria
	- No se sabe con certeza el valor que tomará, sólo qué valores puede tomar y la probabilidad de cada uno de ellos
	- Distribución de probabilidad
		- Conjunto de valores que puede tomar una variable aleatoria y sus probabilidades
		- Histograma -> representación gráfica
	- Parámetros
		- Esperanza matemática o valor esperado
			- Media ponderada de los valores que puede tomar la variable
			- $E(x) = x_1 p_1 + x_2 p_2 + ... + x_n p_n = \bar{x}$
			- Señala dónde se centra la distribución de probabilidad
		- Varianza
			- Esperanza matemática de los cuadrados de las desviaciones de los valores respecto a su media
			- Da idea de la forma de la distribución de probabilidad
			- Unidad: cuadrado de la unidad de la variable
		- Desviación típica
			- Raíz cuadrada positiva de la varianza
			- Misma unidad que la variable
		- Coeficiente de variación
			- Desviación típica por unidad de valor esperado
## El análisis bayesiano
- Teorema de Bayes - (s. XVIII) probabilidad de que ocurra un suceso cuando se conoce que ha sucedido otro. 
- Permite modificar las probabilidades de los distintos estados en función de la información adicional de que se va disponiendo
- $P(S_i|T) = \frac{P(T|S_i)P(S_i)}{P(T|S_1)P(S_1) + P(T|S_2)P(S_2) + ... + P(T|S_3)P(S_3)}$
## La determinación del grado de confianza
- Variables discretas y continuas
- Distribución normal
- Teorema fundamental del límite
- Distribución normal tipificada o estandarizada
## La teoría de la información
- Claude Shannon
- Información proporcionada por la materialización de un suceso 
	- es mayor cuanto menor fuera su probabilidad
	- $h(P)$
		- Caracterización
			- Decreciente con P
			- Tiende a infinito cuando P tiende a 0
			- Es 0 cuando P es 1
			- Monótona y continua
			- La información proporcionada por la ocurrencia conjunta de dos sucesos es la suma de las informaciones proporcionadas por la ocurrencia de cada uno de ellos
		- Se induce: $h(P) = log \frac{1}{P} = -log P$
		- El logaritmo puede ser
			- neperiano -> unidad *nits*
			- binario -> unidad *bits*
			- decimal -> unidad *hartleys*	
	- Entropía de un sistema de sucesos
		- esperanza matemática de las informaciones aportadas por cada uno de los sucesos
		- $H = P_1 h(P_1) + P_2 h(P_2) + ... + P_n h(P_n)$ ->
			- -> $H = -[P_1 log(P_1) + P_2 log(P_2) + ... + P_n log(P_n)$ 
		- Mide la incertidumbre del sistema
		- siempre no negativa
		- Es máxima cuanto más igualadas están las probabilidades de los sucesos, y mínima cuanto más probabilidad tiene un suceso concreto
- Contenido informativo de un mensaje
	- Medida de la información proporcionada por la variación de probabilidad (de $P_j$ a $Q_j$) de un suceso a consecuencia del conocimiento de dicho mensaje
	- $h(P_j) - h(Q_j) = - log(P_j) + log(Q_j) = log (\frac{Q_j}{P_j})$ 
- Información de canal
	- contenido informativo de un mensaje sobre un sistema de sucesos
	- $I(Q:P) = Q_1 log (\frac{Q_1}{P_1}) + Q_2 log (\frac{Q_2}{P_2}) + ... + Q_n log (\frac{Q_n}{P_n})$
